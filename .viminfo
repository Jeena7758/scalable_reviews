# This viminfo file was generated by Vim 9.1.
# You may edit it if you're careful!

# Viminfo version
|1,4

# Value of 'encoding' when this file was written
*encoding=utf-8


# hlsearch on (H) or off (h):
~h
# Last Search Pattern:
~MSle0~/total_records_processed

# Command Line History (newest to oldest):
:q
|2,0,1751486576,,"q"
:wq!
|2,0,1751486561,,"wq!"
:q!
|2,0,1751450291,,"q!"
:wq1
|2,0,1751386587,,"wq1"
:950
|2,0,1751149172,,"950"
:set nu
|2,0,1751149168,,"set nu"
:Q
|2,0,1751046366,,"Q"

# Search String History (newest to oldest):
?/total_records_processed
|2,1,1751461739,47,"total_records_processed"
?/message.max.bytes
|2,1,1751450409,47,"message.max.bytes"
?/message.max
|2,1,1751450397,47,"message.max"
?/message_max
|2,1,1751450388,47,"message_max"
?/socket.request.max.bytes
|2,1,1751450322,47,"socket.request.max.bytes"
?/messages_to_p
|2,1,1751408127,47,"messages_to_p"
?/chunk
|2,1,1751407299,47,"chunk"
?/chunksize
|2,1,1751407140,47,"chunksize"
?/consumer
|2,1,1751406922,47,"consumer"
?/zookeeper.con
|2,1,1751405586,47,"zookeeper.con"
?/listeners
|2,1,1751405383,47,"listeners"
?/batch_size
|2,1,1751403291,47,"batch_size"
?/KafkaServer id
|2,1,1751383186,47,"KafkaServer id"
?/INFO [KafkaServer id=0] started
|2,1,1751382970,47,"INFO [KafkaServer id=0] started"
?/INFO Registered broker 0 with zookeeper
|2,1,1751382959,47,"INFO Registered broker 0 with zookeeper"
?/linger
|2,1,1751368150,47,"linger"
?/batch
|2,1,1751367988,47,"batch"
?/timeouti
|2,1,1751325933,47,"timeouti"
?/Reading
|2,1,1751324611,47,"Reading"
?/hread {os.getpid()}: Reading
|2,1,1751324544,47,"hread {os.getpid()}: Reading"
?/reading
|2,1,1751324500,47,"reading"
?/pid
|2,1,1751324474,47,"pid"
?/read {os.getpid()}: Reading
|2,1,1751324414,47,"read {os.getpid()}: Reading"
?/current_thread
|2,1,1751324305,47,"current_thread"
?/getpid
|2,1,1751324057,47,"getpid"
?/thread.pid
|2,1,1751323990,47,"thread.pid"
?/producer.se
|2,1,1751323729,47,"producer.se"
?/id
|2,1,1751323049,47,"id"
?/nltk
|2,1,1751322803,47,"nltk"
?/offset
|2,1,1751111011,47,"offset"
?/duration_sec
|2,1,1750976843,47,"duration_sec"
?/time.time() - 
|2,1,1750974590,47,"time.time() - "
?/total_reviews
|2,1,1750972148,47,"total_reviews"
?/metric
|2,1,1750972105,47,"metric"
?/BATCH_THREAD_SIZE
|2,1,1750971773,47,"BATCH_THREAD_SIZE"
?/raw_batch
|2,1,1750971736,47,"raw_batch"
?/executor
|2,1,1750971673,47,"executor"
?/features
|2,1,1750971662,47,"features"
?/poll
|2,1,1750971621,47,"poll"
?/THREAD
|2,1,1750971479,47,"THREAD"
?/clear
|2,1,1750968705,47,"clear"

# Expression History (newest to oldest):

# Input Line History (newest to oldest):

# Debug Line History (newest to oldest):

# Registers:
"2	LINE	0
	import boto3
	import json
	from collections import Counter, defaultdict
	import re
	
	S3_BUCKET = 'imdbreviews-scalable'
	S3_PREFIX = 'kafka-consumer-outputs/'
	
	s3 = boto3.client('s3')
	
	def list_json_objects(bucket, prefix):
	    objects = []
	    paginator = s3.get_paginator('list_objects_v2')
	    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
	        if 'Contents' in page:
	            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))
	    return objects
	
	def load_json_from_s3(key):
	    response = s3.get_object(Bucket=S3_BUCKET, Key=key)
	    return json.loads(response['Body'].read().decode('utf-8'))
	
	def extract_timestamp_from_key(key):
	    match = re.search(r'summary_(\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}-\d+)\.json', key)
	    return match.group(1) if match else ""
	
	def summarize_outputs():
	    keys = sorted(list_json_objects(S3_BUCKET, S3_PREFIX), key=extract_timestamp_from_key)
	
	    max_total_records_processed_overall = 0
	    total_sentiment_weighted = 0.0
	    total_reviews_for_sentiment = 0
	    total_duration = 0.0
	    total_records = 0
	    total_run_time_sec = 0.0
	
	    total_positive = 0
	    total_neutral = 0
	    total_negative = 0
	
	    word_counter = Counter()
	    movie_sentiments = defaultdict(list)
	
	    for key in keys:
	        data = load_json_from_s3(key)
	
	        current_total = data.get('total_records_processed_overall', 0)
	        if current_total > max_total_records_processed_overall:
	            max_total_records_processed_overall = current_total
	
|3,0,2,1,50,0,1751464623,"import boto3","import json","from collections import Counter, defaultdict","import re","","S3_BUCKET = 'imdbreviews-scalable'","S3_PREFIX = 'kafka-consumer-outputs/'","","s3 = boto3.client('s3')","","def list_json_objects(bucket, prefix):","    objects = []","    paginator = s3.get_paginator('list_objects_v2')","    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):","        if 'Contents' in page:",>100
|<"            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))","    return objects","","def load_json_from_s3(key):","    response = s3.get_object(Bucket=S3_BUCKET, Key=key)","    return json.loads(response['Body'].read().decode('utf-8'))","","def extract_timestamp_from_key(key):","    match = re.search(r'summary_(\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}-\\d+)\\.json', key)","    return match.group(1) if match else \"\"","",>26
|<"def summarize_outputs():","    keys = sorted(list_json_objects(S3_BUCKET, S3_PREFIX), key=extract_timestamp_from_key)","","    max_total_records_processed_overall = 0","    total_sentiment_weighted = 0.0","    total_reviews_for_sentiment = 0","    total_duration = 0.0","    total_records = 0","    total_run_time_sec = 0.0","","    total_positive = 0","    total_neutral = 0","    total_negative = 0","","    word_counter = Counter()","    movie_sentiments = defaultdict(list)","",>22
|<"    for key in keys:","        data = load_json_from_s3(key)","","        current_total = data.get('total_records_processed_overall', 0)","        if current_total > max_total_records_processed_overall:","            max_total_records_processed_overall = current_total",""
"3	LINE	0
	import boto3
	import json
	import time
	import re
	from collections import Counter, defaultdict
	
	S3_BUCKET = 'imdbreviews-scalable'
	S3_PREFIX = 'kafka-consumer-outputs/'
	
	s3 = boto3.client('s3')
	
	def list_json_objects(bucket, prefix):
	    objects = []
	    paginator = s3.get_paginator('list_objects_v2')
	    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
	        if 'Contents' in page:
	            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))
	    return objects
	
	def load_json_from_s3(key):
	    response = s3.get_object(Bucket=S3_BUCKET, Key=key)
	    return json.loads(response['Body'].read().decode('utf-8'))
	
	def summarize_outputs():
	    keys = list_json_objects(S3_BUCKET, S3_PREFIX)
	    print(f"Found {len(keys)} summary files")
	
	    max_total_records_processed_overall = 0
	    weighted_sentiment_sum = 0.0
	    total_records_for_averages = 0
	    total_run_time_sec = 0.0
	    word_counter = Counter()
	    movie_sentiments = defaultdict(list)
	    total_positive = 0
	    total_neutral = 0
	    total_negative = 0
	
	    def extract_timestamp_from_key(key):
	        match = re.search(r'summary_(\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}-\d+)\.json', key)
	        return match.group(1) if match else ""
	
	    keys.sort(key=extract_timestamp_from_key)
	
	    for key in keys:
	        data = load_json_from_s3(key)
	
	        current_overall_records = data.get('total_records_processed_overall', 0)
	        if current_overall_records > max_total_records_processed_overall:
	            max_total_records_processed_overall = current_overall_records
	
|3,0,3,1,50,0,1751464080,"import boto3","import json","import time","import re","from collections import Counter, defaultdict","","S3_BUCKET = 'imdbreviews-scalable'","S3_PREFIX = 'kafka-consumer-outputs/'","","s3 = boto3.client('s3')","","def list_json_objects(bucket, prefix):","    objects = []","    paginator = s3.get_paginator('list_objects_v2')","    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):","        if 'Contents' in page:",>100
|<"            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))","    return objects","","def load_json_from_s3(key):","    response = s3.get_object(Bucket=S3_BUCKET, Key=key)","    return json.loads(response['Body'].read().decode('utf-8'))","","def summarize_outputs():","    keys = list_json_objects(S3_BUCKET, S3_PREFIX)","    print(f\"Found {len(keys)} summary files\")","","    max_total_records_processed_overall = 0",>34
|<"    weighted_sentiment_sum = 0.0","    total_records_for_averages = 0","    total_run_time_sec = 0.0","    word_counter = Counter()","    movie_sentiments = defaultdict(list)","    total_positive = 0","    total_neutral = 0","    total_negative = 0","","    def extract_timestamp_from_key(key):","        match = re.search(r'summary_(\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}-\\d+)\\.json', key)","        return match.group(1) if match else \"\"","",>47
|<"    keys.sort(key=extract_timestamp_from_key)","","    for key in keys:","        data = load_json_from_s3(key)","","        current_overall_records = data.get('total_records_processed_overall', 0)","        if current_overall_records > max_total_records_processed_overall:","            max_total_records_processed_overall = current_overall_records",""
"4	LINE	0
	import boto3
	import json
	import time
	from collections import Counter, defaultdict
	import re # Import regex for timestamp extraction
	
	S3_BUCKET = 'imdbreviews-scalable'
	S3_PREFIX = 'kafka-consumer-outputs/'
	
	s3 = boto3.client('s3')
	
	def list_json_objects(bucket, prefix):
	    objects = []
	    paginator = s3.get_paginator('list_objects_v2')
	    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
	        if 'Contents' in page:
	            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))
	    return objects
	
	def load_json_from_s3(key):
	    response = s3.get_object(Bucket=S3_BUCKET, Key=key)
	    return json.loads(response['Body'].read().decode('utf-8'))
	
	def summarize_outputs():
	    keys = list_json_objects(S3_BUCKET, S3_PREFIX)
	    print(f"Found {len(keys)} summary files")
	
	    max_total_records_processed_overall = 0 # To store the ultimate total reviews
	
	    total_sentiment = 0.0
	    total_duration = 0.0
	
	    # These will now track weighted averages or sums
	    weighted_sentiment_sum = 0.0
	    total_records_for_averages = 0 # To track total records contributing to avg sentiment/spoiler
	
	    # For avg latency and throughput, we need to sum up individual batch durations and records
	    total_records_for_avg_latency_throughput = 0
	    total_duration_for_avg_latency_throughput = 0.0
	
	
	    word_counter = Counter()
	    movie_sentiments = defaultdict(list)
	
	    # To track the last seen overall records for correct total
	    last_overall_records = 0
	
	    # Sort keys by timestamp to ensure we process in order and get the latest overall count
	    # Assuming filenames are like summary_2025-06-28T17-02-36-350486.json
	    def extract_timestamp_from_key(key):
|3,0,4,1,50,0,1751463851,"import boto3","import json","import time","from collections import Counter, defaultdict","import re # Import regex for timestamp extraction","","S3_BUCKET = 'imdbreviews-scalable'","S3_PREFIX = 'kafka-consumer-outputs/'","","s3 = boto3.client('s3')","","def list_json_objects(bucket, prefix):","    objects = []","    paginator = s3.get_paginator('list_objects_v2')","    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):",>32
|<"        if 'Contents' in page:","            objects.extend(obj['Key'] for obj in page['Contents'] if obj['Key'].endswith('.json'))","    return objects","","def load_json_from_s3(key):","    response = s3.get_object(Bucket=S3_BUCKET, Key=key)","    return json.loads(response['Body'].read().decode('utf-8'))","","def summarize_outputs():","    keys = list_json_objects(S3_BUCKET, S3_PREFIX)","    print(f\"Found {len(keys)} summary files\")","",>83
|<"    max_total_records_processed_overall = 0 # To store the ultimate total reviews","","    total_sentiment = 0.0","    total_duration = 0.0","","    # These will now track weighted averages or sums","    weighted_sentiment_sum = 0.0","    total_records_for_averages = 0 # To track total records contributing to avg sentiment/spoiler","","    # For avg latency and throughput, we need to sum up individual batch durations and records","    total_records_for_avg_latency_throughput = 0",>53
|<"    total_duration_for_avg_latency_throughput = 0.0","","","    word_counter = Counter()","    movie_sentiments = defaultdict(list)","","    # To track the last seen overall records for correct total","    last_overall_records = 0","","    # Sort keys by timestamp to ensure we process in order and get the latest overall count","    # Assuming filenames are like summary_2025-06-28T17-02-36-350486.json","    def extract_timestamp_from_key(key):"
"5	LINE	0
	        'avg_spoiler_percent': round(100 * weighted_spoiler_count_sum / total_records_for_averages, 2) if total_records_for_averages else None,
|3,0,5,1,1,0,1751463430,"        'avg_spoiler_percent': round(100 * weighted_spoiler_count_sum / total_records_for_averages, 2) if total_records_for_averages else None,"
"6	LINE	0
	            # No need to add to total_records_for_averages again, it's the same base count
|3,0,6,1,1,0,1751463354,"            # No need to add to total_records_for_averages again, it's the same base count"
"7	LINE	0
	            weighted_spoiler_count_sum += (spoiler_pct / 100) * batch_reviews # Convert percentage to a proportion of reviews
|3,0,7,1,1,0,1751463352,"            weighted_spoiler_count_sum += (spoiler_pct / 100) * batch_reviews # Convert percentage to a proportion of reviews"
"8	LINE	0
	        if spoiler_pct is not None and batch_reviews > 0:
|3,0,8,1,1,0,1751463351,"        if spoiler_pct is not None and batch_reviews > 0:"
"9	LINE	0
	        spoiler_pct = data.get('spoiler_percent_window')
|3,0,9,1,1,0,1751463350,"        spoiler_pct = data.get('spoiler_percent_window')"
"-	CHAR	0
	'
|3,0,36,0,1,0,1751326969,"'"

# File marks:
'0  12  4  ~/.gitignore
|4,48,12,4,1751486576,"~/.gitignore"
'1  12  4  ~/.gitignore
|4,49,12,4,1751486561,"~/.gitignore"
'2  1  0  ~/1
|4,50,1,0,1751486479,"~/1"
'3  1  0  ~/1
|4,51,1,0,1751486472,"~/1"
'4  267  0  ~/imdb_consumer_kafka.py
|4,52,267,0,1751486446,"~/imdb_consumer_kafka.py"
'5  12  0  ~/mapreduce_hashtag.py
|4,53,12,0,1751486219,"~/mapreduce_hashtag.py"
'6  1  0  ~/mapreduce_hashtag
|4,54,1,0,1751486208,"~/mapreduce_hashtag"
'7  36  4  ~/cleaning.py
|4,55,36,4,1751466786,"~/cleaning.py"
'8  129  30  ~/file_cleaning.py
|4,56,129,30,1751466750,"~/file_cleaning.py"
'9  106  0  ~/imdb_summary.py
|4,57,106,0,1751464627,"~/imdb_summary.py"

# Jumplist (newest first):
-'  12  4  ~/.gitignore
|4,39,12,4,1751486576,"~/.gitignore"
-'  12  4  ~/.gitignore
|4,39,12,4,1751486561,"~/.gitignore"
-'  11  19  ~/.gitignore
|4,39,11,19,1751486529,"~/.gitignore"
-'  11  19  ~/.gitignore
|4,39,11,19,1751486529,"~/.gitignore"
-'  1  0  ~/1
|4,39,1,0,1751486479,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486479,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486479,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486479,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486472,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486472,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486472,"~/1"
-'  1  0  ~/1
|4,39,1,0,1751486472,"~/1"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  267  0  ~/imdb_consumer_kafka.py
|4,39,267,0,1751486446,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  1  0  ~/imdb_consumer_kafka.py
|4,39,1,0,1751486344,"~/imdb_consumer_kafka.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  12  0  ~/mapreduce_hashtag.py
|4,39,12,0,1751486219,"~/mapreduce_hashtag.py"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"
-'  1  0  ~/mapreduce_hashtag
|4,39,1,0,1751486208,"~/mapreduce_hashtag"

# History of marks within files (newest to oldest):

> ~/.gitignore
	*	1751486570	0
	"	12	4
	^	12	5
	.	12	4
	+	6	41
	+	4	0
	+	7	4
	+	4	0
	+	6	11
	+	8	17
	+	10	7
	+	11	20
	+	12	4

> ~/1
	*	1751486477	0
	"	1	0

> ~/imdb_consumer_kafka.py
	*	1751486443	0
	"	267	0
	^	267	10
	.	267	9
	+	1	9
	+	1	10
	+	1	0
	+	1	9
	+	1	10
	+	1	20
	+	1	9
	+	1	10
	+	1	15
	+	1	34
	+	1	38
	+	1	14
	+	1	38
	+	1	64
	+	1	23
	+	1	50
	+	1	9
	+	1	25
	+	1	20
	+	1	12
	+	1	11
	+	1	12
	+	1	9
	+	1	11
	+	1	57
	+	1	63
	+	1	14
	+	1	0
	+	1	9
	+	1	0
	+	1	64
	+	1	9
	+	267	9

> ~/mapreduce_hashtag.py
	*	1751486210	0
	"	12	0
	^	12	31
	.	12	30
	+	1	12
	+	1	11
	+	1	12
	+	1	32
	+	1	21
	+	1	0
	+	1	29
	+	1	19
	+	1	9
	+	1	19
	+	1	0
	+	1	18
	+	1	13
	+	1	18
	+	1	96
	+	1	0
	+	1	12
	+	1	32
	+	1	21
	+	1	59
	+	1	4
	+	1	73
	+	1	22
	+	1	11
	+	1	22
	+	1	11
	+	1	12
	+	1	13
	+	1	14
	+	1	12
	+	1	14
	+	1	13
	+	1	12
	+	1	48
	+	1	54
	+	1	39
	+	1	22
	+	1	6
	+	1	53
	+	1	83
	+	1	34
	+	1	10
	+	1	2
	+	1	16
	+	1	18
	+	1	5
	+	1	12
	+	137	53
	+	12	30

> ~/mapreduce_hashtag
	*	1751486205	0
	"	1	0

> ~/cleaning.py
	*	1751466777	0
	"	36	4
	^	112	47
	.	112	46
	+	112	46

> ~/file_cleaning.py
	*	1751466748	0
	"	129	30
	^	11	31
	.	11	30
	+	1	7
	+	1	90
	+	1	32
	+	1	25
	+	1	12
	+	130	18
	+	6	0
	+	4	10
	+	5	9
	+	4	10
	+	11	30

> ~/imdb_summary.py
	*	1751464625	0
	"	106	0
	^	106	0
	.	105	23
	+	1	23
	+	1	67
	+	1	66
	+	1	0
	+	1	49
	+	1	67
	+	1	0
	+	1	92
	+	1	12
	+	1	23
	+	1	13
	+	1	12
	+	1	23
	+	1	16
	+	1	18
	+	1	10
	+	1	0
	+	1	12
	+	1	18
	+	1	12
	+	105	23

> ~/producer2.py
	*	1751456949	0
	"	1	0
	^	103	0
	.	102	89
	+	1	12
	+	1	0
	+	1	32
	+	1	12
	+	102	89

> ~/kafka_2.13-3.7.0/config/server.properties
	*	1751450435	0
	"	59	26
	^	59	27
	.	59	26
	+	38	45
	+	34	36
	+	35	45
	+	58	33
	+	59	26

> ~/kafka_2.13-3.7.0/logs/server.log
	*	1751450289	0
	"	1268	0
	^	1268	1

> ~/producer.py
	*	1751448605	0
	"	101	27
	^	101	28
	.	101	27
	+	142	9
	+	14	37
	+	15	29
	+	14	39
	+	12	18
	+	11	35
	+	29	37
	+	124	12
	+	65	27
	+	121	96
	+	122	56
	+	124	0
	+	65	8
	+	101	27

> ~/consumer.py
	*	1751444708	0
	"	1	0
	^	1	0

> ~/imdb_consumer_kafka_new.py
	*	1751408160	0
	"	263	0
	^	169	99
	.	169	99
	+	643	63
	+	585	22
	+	1	21
	+	1	0
	+	1	27
	+	1	0
	+	1	36
	+	1	0
	+	1	9
	+	1	21
	+	1	9
	+	262	10
	+	26	13
	+	20	30
	+	18	31
	+	19	22
	+	128	4
	+	135	0
	+	134	46
	+	129	7
	+	130	7
	+	131	7
	+	132	7
	+	133	7
	+	169	99

> ~/imdb_producer_s3.py
	*	1751407503	0
	"	29	50
	^	29	51
	.	132	96
	+	1	83
	+	1	12
	+	1	94
	+	1	14
	+	1	11
	+	1	12
	+	1	11
	+	1	16
	+	1	1
	+	1	20
	+	1	2
	+	1	12
	+	1	90
	+	1	12
	+	203	0
	+	1	12
	+	1	90
	+	1	29
	+	1	25
	+	1	28
	+	1	16
	+	1	12
	+	1	18
	+	1	19
	+	1	0
	+	1	95
	+	1	44
	+	1	61
	+	1	94
	+	1	15
	+	1	24
	+	1	7
	+	1	77
	+	1	15
	+	1	44
	+	1	45
	+	1	53
	+	1	25
	+	1	0
	+	1	1
	+	1	12
	+	1	14
	+	1	24
	+	1	11
	+	1	12
	+	1	11
	+	1	16
	+	1	20
	+	1	25
	+	1	26
	+	1	32
	+	1	9
	+	1	67
	+	1	18
	+	1	16
	+	1	31
	+	1	27
	+	1	16
	+	1	12
	+	1	93
	+	1	32
	+	1	25
	+	1	20
	+	1	16
	+	1	11
	+	1	12
	+	1	11
	+	1	24
	+	1	12
	+	132	96

> ~/seq_hashtag.py
	*	1751329022	0
	"	159	37
	^	159	38
	.	159	37
	+	1	14
	+	1	12
	+	1	81
	+	1	0
	+	1	12
	+	1	31
	+	1	19
	+	1	0
	+	1	12
	+	159	37

> ~/seq_sentiment.py
	*	1751328480	0
	"	20	19
	^	20	20
	.	20	20
	+	1	14
	+	1	9
	+	1	78
	+	1	0
	+	1	9
	+	224	37
	+	20	20

> ~/seq_wordcount.py
	*	1751327918	0
	"	8	19
	^	8	20
	.	8	19
	+	1	12
	+	1	32
	+	1	0
	+	1	12
	+	139	31
	+	125	13
	+	117	13
	+	108	11
	+	102	11
	+	100	12
	+	98	11
	+	90	14
	+	32	16
	+	35	15
	+	8	19

> ~/mapreduce_sentiment_new.py
	*	1751322803	0
	"	220	4
	^	16	20
	.	16	19
	+	1	84
	+	1	0
	+	1	9
	+	1	84
	+	1	9
	+	220	18
	+	16	19

> ~/mapreduce_hashtag_new.py
	*	1751321179	0
	"	1	0
	.	26	0
	+	213	18
	+	18	35
	+	21	0
	+	26	0

> ~/mapreduce_wordcount_new.py
	*	1751320191	0
	"	41	25
	^	18	26
	.	18	25
	+	227	56
	+	1	8
	+	18	25

> ~/file_split.py
	*	1751319593	0
	"	8	31
	^	8	32
	.	8	31
	+	34	41
	+	8	31

> ~/mapreduce_sentiment.py
	*	1751290861	0
	"	13	24
	^	13	25
	.	13	24
	+	1	12
	+	1	11
	+	1	82
	+	1	21
	+	1	0
	+	1	22
	+	1	0
	+	1	10
	+	1	14
	+	1	11
	+	1	72
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	14
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	20
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	21
	+	1	12
	+	1	14
	+	1	11
	+	1	16
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	9
	+	1	12
	+	1	14
	+	1	12
	+	1	9
	+	1	12
	+	1	9
	+	1	12
	+	1	14
	+	1	13
	+	1	12
	+	1	11
	+	1	12
	+	1	9
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	9
	+	1	12
	+	1	11
	+	1	12
	+	1	11
	+	1	12
	+	1	71
	+	1	9
	+	1	78
	+	1	16
	+	1	44
	+	1	0
	+	1	27
	+	1	34
	+	1	43
	+	1	45
	+	1	23
	+	1	37
	+	1	52
	+	1	0
	+	1	34
	+	1	73
	+	1	55
	+	1	6
	+	1	0
	+	1	34
	+	1	72
	+	1	5
	+	1	90
	+	1	59
	+	1	16
	+	1	18
	+	1	9
	+	130	47
	+	15	30
	+	13	24

> ~/mapreduce_wordcount.py
	*	1751227356	0
	"	133	0
	^	11	40
	.	11	39
	+	1	12
	+	1	33
	+	1	71
	+	1	22
	+	1	0
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	13
	+	1	12
	+	1	14
	+	1	13
	+	1	11
	+	1	12
	+	1	14
	+	1	12
	+	1	11
	+	1	16
	+	1	12
	+	1	14
	+	1	11
	+	1	16
	+	1	12
	+	1	32
	+	1	12
	+	1	14
	+	1	12
	+	132	63
	+	11	39

> ~/visualization.py
	*	1751226918	0
	"	33	0
	^	11	34
	.	11	34
	+	1	9
	+	1	7
	+	1	16
	+	1	7
	+	1	0
	+	1	59
	+	1	7
	+	1	0
	+	1	9
	+	91	39
	+	11	34

> ~/mapreduce_se
	*	1751046366	0
	"	1	0

> ~/mapreduce_Se
	*	1751042850	0
	"	1	0
